3) 머하웃과 스파크ML을 이용한 머신러닝
 : 머하웃과 스파크ML 같은 머신러닝 기술은 복잡도가 높은 비즈니스 로직을 자동으로 생성 및 관리하거나, 
   대규모 단순 반복 작업에서 패턴들을 찾아 효율화하는 데 사용 된다. 이때 자동으로 만들어진 프로그램을 모델이라고 하며, 
   모델은 대규모 데이터에서 과거의 패턴을 찾아 정의하는 학습 과정을 통해 만들어진다. 
   학습이 완료된 모델에 현재의 데이터를 입력해서 앞으로 발생할 일들을 예측하면서 신속한 의사결정을 내리도록 지원한다.

 # 실습 (머하웃 추천 - 스마트카 차량용품 추천)
   Managed SmartCar_Item BuyList Info 테이블에 약 10만 건의 데이터가 적재돼 있다.
   추천에 필요한 항목은 3개의 필드로서 차량고유번호(car_number), 구매용품아이템코드(item), 사용평가점수(score)다. 
   유사 패턴을 보이는 사용자 간의 유사성을 계산하고, 그 결과로부터 유사 사용자 간의 선호하는 아이템을 예측해서 
   추천하는 것이 사용자 기반 협업 필터링 모델이다
  - hive QL에 아래의 sql을 적는다
  - insert overwrite local directory '/home/pilot-pjt/mahout-data/recommendation/input'
    ROW FORMAT DELIMITED
    FIELDS TERMINATED BY ','
    select hash(car_number), hash(item), score from managed_smartcar_item_buylist_info
    ** hash() 고유값으로 전달하기 위해 쓰는 함수
  - 확인 작업
    [root@server02 ~]# cd /home/pilot-pjt/mahout-data/recommendation/input/
    [root@server02 input]# ll
    total 2024
    -rw-r--r-- 1 hive hive 2071916 May  3 10:16 000000_0
    [root@server02 input]# more /home/pilot-pjt/mahout-data/recommendation/input/*
  - 머하웃 추천 실행
    mahout recommenditembased -i /pilot-pjt/mahout/recommendation/input/item_buylist.txt -o /pilot-pjt/mahout/recommendation/output/ -s SIMILARITY_COOCCURRENCE -n 3
    INFO MahoutDriver: Program took 451949 ms (Minutes: 7.532483333333333) <- 이문장 나오면 끝
  - 휴의 파일브라우저에서 / pilot-pjt/ mahout/ recommendation/ output/ part-r-00000를 찾아가면 만들어진 것을 볼 수 있다.
  - 추천 분석을 재실행할 때는 기존 결과 파일을 삭제한 후 재실행해야 한다.
    hdfs dfs -rm -R -skipTrash /pilot-pjt/mahout/recommendation/output
    hdfs dfs -rm -R -skipTrash /user/root/temp

  # 실습 (스파크ML 분류 - 스마트카 상태 정보 예측/분류)
   데이터 마이닝의 분류에 사용될 데이터셋은 “스마트카 상태 정보”로 하이브의 Manage 영역에 
   Managed SmartCar Status Info라는 이름의 테이블에 약 200만 건의 데이터가 적재돼 있다.
   스마트카의 주요 장치(타이어, 라이트, 엔진, 브레이크 등)에 대한 상태를 기록한 값으로, 차량의 상태를 진단하기 위한 중요한 변수다. 
   이 값들을 이용해 차량의 정상/비정상을 분류하는 모델을 만들고, 이 분류 모델을 운행 중인 스마트카에 적용해서 차량의 안전 상태를 
   실시간으로 점검하는 머신러닝 분석이 이번 실습의 목표다.
   분류 모델에 사용하는 알고리즘으로 나이브 베이지안, 랜덤 포레스트, 로지스틱 회귀 등이 있으나 
   이번 파일럿 프로젝트에서는 랜덤 포레스트를 이용한다.
   ** 과거 수개월의 스마트카 운행 데이터를 스파크 머신러닝 분류기에 입력해 분류기를 훈련(학습) 시킨다. 
    학습이 끝나면 스마트카의 상태를 판단할 수 있는 Classify(or Molel) 프로그램이 만들어지고, 이 프로그램 안에는 
    과기의 데이터로부터 학습해 찾아낸 분류 패턴들이 로직화되어 들어가 있다. 이 Classify 프로그램을 운행 중인 
    스마트카 시스템에 적용하고, 스마트카에서 발생하는 데이터를 Classify 프로그램이 분석해 가며 과거의 이상 패턴과 0
    유사한 데이터가 발생하는지 실시간으로 분류 및 예측하게 된다.
  - 스마트카 상태 정보 데이터셋 가공
insert overwrite local directory '/home/pilot-pjt/spark-data/classification/input'
ROW FORMAT DELIMITED
FIELDS TERMINATED BY ','
select
  sex, age, marriage, region, job, car_capacity, car_year, car_model,
  tire_fl, tire_fr, tire_bl, tire_br, light_fl, light_fr, light_bl, light_br,
  engine, break, battery,
  case when ((tire_fl_s + tire_fr_s + tire_bl_s + tire_br_s +
              light_fl_s + light_fr_s + light_bl_s + light_br_s +
              engine_s + break_s + battery_s +
              car_capacity_s + car_year_s + car_model_s) < 6)
        then '비정상' else '정상'
  end as status
from (
  select
    sex, age, marriage, region, job, car_capacity, car_year, car_model,
    tire_fl, tire_fr, tire_bl, tire_br, light_fl, light_fr, light_bl, light_br,
    engine, break, battery,

    case
     when (1500 > cast(car_capacity as int)) then -0.3
        when (2000 > cast(car_capacity as int)) then -0.2
        else -0.1
    end as car_capacity_s ,
    
    case
    when (2005 > cast(car_year as int)) then -0.3
        when (2010 > cast(car_year as int)) then -0.2
        else -0.1
    end as car_year_s,

    case
    when ('B' = car_model) then -0.3
        when ('D' = car_model) then -0.3
        when ('F' = car_model) then -0.3
        when ('H' = car_model) then -0.3
        else 0.0
    end as car_model_s ,

    case
        when (10 > cast(tire_fl as int)) then 0.1
        when (20 > cast(tire_fl as int)) then 0.2
        when (40 > cast(tire_fl as int)) then 0.4
        else 0.5
    end as tire_fl_s,
    
    case
        when (10 > cast(tire_fr as int)) then 0.1
        when (20 > cast(tire_fr as int)) then 0.2
        when (40 > cast(tire_fr as int)) then 0.4
        else 0.5
    end as tire_fr_s ,

    case
        when (10 > cast(tire_bl as int)) then 0.1
        when (20 > cast(tire_bl as int)) then 0.2
        when (40 > cast(tire_bl as int)) then 0.4
        else 0.5
    end as tire_bl_s ,
    
    case
        when (10 > cast(tire_br as int)) then 0.1
        when (20 > cast(tire_br as int)) then 0.2
        when (40 > cast(tire_br as int)) then 0.4
        else 0.5
    end as tire_br_s ,

    case when (cast(light_fl as int) = 2) then 0.0 else 0.5 end as light_fl_s,
    case when (cast(light_fr as int) = 2) then 0.0 else 0.5 end as light_fr_s,
    case when (cast(light_bl as int) = 2) then 0.0 else 0.5 end as light_bl_s,
    case when (cast(light_br as int) = 2) then 0.0 else 0.5 end as light_br_s,

    case
        when (engine = 'A') then 1.0
        when (engine = 'B') then 0.5
        when (engine = 'C') then 0.0
    end as engine_s ,

    case
        when (break = 'A') then 1.0
        when (break = 'B') then 0.5
        when (break = 'C') then 0.0
    end as break_s ,

    case
        when (20 > cast(battery as int)) then 0.2
        when (40 > cast(battery as int)) then 0.4
        when (60 > cast(battery as int)) then 0.6
        else 1.0
    end as battery_s

from managed_smartcar_status_info ) T1

  - “스마트카 상태 정보" 데이터로부터 예측 변수(차량용량, 차량연식, 차량모델, 타이어, 라이트, 엔진, 브레이크, 배터리)들의 
    상태값으로 필자의 주관적인 기준으로 피처 엔지니어링했다. 각 변수에 대한 정규화 및 스케일링 같은 작업을 대용량 처리가 
    용이한 하이브에서 전처리하면 분석 단계에서 학습 데이터를 다루기가 쉬워져 빠른 분석이 가능해진다. 예측변수에는 
    보정치 변수(마이너스)와 가중치 변수(플러스값)가 있고, 이 모든 예측변수의 값을 합산해서 “6 미만인 경우 “비정상”을 
    “6 이상인 경우에 “정상”인 값을 가지는 목표변수를 정의했다.

  - $ more /home/pilot-pjt/spark-data/classification/input/* <- 데이터셋이 정상적으로 만들어졌는지 확인

  - $ ls /home/pilot-pjt/spark-data/classification/input <- 파일이름 검색
    000000_0  000001_0
  - 분류기의 트레이닝 데이터셋을 만들기 위해 우선 두 개의 파일을 리눅스의 cat 명령을 이용해 하나의 파일로 합쳐 
    classification_dataset.txt라는 이름의 파일을 만든다.
    $ cd /home/pilot-pjt/spark-data/classification/input
    $ cat 000000_0 000001_0 > classification_dataset.txt
      * > : redirect(보낸다) = file에서 w를 의미함
      * >> : append를 의미함
    앞선 cat 명령에서 머지할 파일이 없을 경우 에러 메시지가 나타나는데 이는 무시해도 좋다.
  -  스파크의 입력 데이터로 사용하기 위해 HDFS의 pilot-pit/spark-data/classification/input/ 경로를 생성하고
     classification_dataset.txt 파일을 저장한다.
     $ hdfs dfs -mkdir -p /pilot-pjt/spark-data/classification/input
     $ hdfs dfs -put /home/pilot-pit/spark-data/classification/input/classification_dataset.txt /pilot-pjt/spark-data/classification/input
     $ hdfs dfs -ls /pilot-pjt/spark-data/classification/input <- 정상적으로 옮겨졌는지 확인
  -  스파크ML을 실행하기 위해 제플린을 활용한다. 제플린이 종료됐으면 다음 명령어를 통해 제플린 서버를 실행하고, 
     크롬 브라우저를 통해 제플린 웹DE에 접속한다.
     $ zeppelin-daemon.sh restart
     제플린 웹IDE URL: http://server02.hadoop.com:8081/
  - 제플린 상단 메뉴의 [Nolebook] -> [Create new note]를 선택하고 [Note Name]으로 "SmartcarClassification"을 입력하고
    Default Interpreter 는 "spark"를 선택한 후 [Create] 버튼을 클릭한다.
  - 스파크ML은 크게 자바, 파이썬, 스칼라로 개발할 수 있다. 파일럿 프로젝트에서는 스칼라를 이용하겠다.
   import org.apache.spark.ml.feature.{IndexToString, StringIndexer, VectorIndexer, StringIndexerModel, VectorAssembler}
   import org.apache.spark.ml.feature.MinMaxScaler
   import org.apache.spark.ml.Pipeline
   import org.apache.spark.ml.classification. {RandomForestClassificationModel, RandomForestClassifier} 
   import org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator
   import org.apache.spark.mllib.evaluation. BinaryClassificationMetrics
   import org.apache.spark.mllib.evaluation.MulticlassMetrics
   import org.apache.spark.mllib.util.MLUtils
  - 5개의 데이터 조회
    val ds = spark.read.csv("/pilot-pjt/spark-data/classification/input/classification_dataset.txt")
    ds.show(5)
  - 이상징후 탐지 모델에 사용할 컬럼만 선택해 스파크 데이터셋을 새로 만든다
    val dsSmartCar = ds.selectExpr("cast(_c5 as long) car_capacity", 
                        "cast(_c6 as long) car_year", 
                        "cast(_c7 as string) car_model",
                        "cast(_c8 as int) tire_fl",
                        "cast(_c9 as long) tire_fr",
                        "cast(_c10 as long) tire_bl",
                        "cast(_c11 as long) tire_br",
                        "cast(_c12 as long) light_f1",
                        "cast(_c13 as long) light_fr",
                        "cast(_c14 as long) light_bl",
                        "cast(_c15 as long) light_br", 
                        "cast(_c16 as string) engine",
                        "cast(_c17 as string) break",
                        "cast(_c18 as long) battery",
                        "cast(_c19 as string) status"
                        )
  - 문자형 카테고리 칼럼을 숫자형 칼럼으로 생성하고, 기존 칼럼은 삭제한다.
    val dsSmartCar_1 = new StringIndexer().setInputCol("car_model").setOutputCol("car_model_n").fit(dsSmartCar).transform(dsSmartCar);
    val dsSmartCar_2 = new StringIndexer().setInputCol("engine").setOutputCol("engine_n").fit(dsSmartCar_1).transform(dsSmartCar_1);
    val dsSmartCar_3 = new StringIndexer().setInputCol("break").setOutputCol("break_n").fit(dsSmartCar_2).transform(dsSmartCar_2);
    val dsSmartCar_4 = new StringIndexer().setInputCol("status").setOutputCol("label").fit(dsSmartCar_3).transform(dsSmartCar_3);
    val dsSmartCar_5 = dsSmartCar_4.drop("car_model").drop("engine").drop("break").drop("status");
    dsSmartCar_5.show()
  - 머신러닝에 사용할 변수를 벡터화해서 feature라는 필드에 새로 생성하고 해당 값들에 대해 스케일링 작업도 진행하는 코드를 실행한다.
    val cols = Array("car_capacity", "car_year", "car_model_n", "tire_fl", "tire_fr", "tire_bl", "tire_br", "light_f1", "light_fr", "light_bl", "light_br", "engine_n", "break_n", "battery")
    val dsSmartCar_6 = new VectorAssembler().setInputCols(cols).setOutputCol("features").transform(dsSmartCar_5)
    val dsSmartCar_7 = new MinMaxScaler().setInputCol("features").setOutputCol("scaledFeatures").fit(dsSmartCar_6).transform(dsSmartCar_6)
    val dsSmartCar_8 = dsSmartCar_7.drop ("features").withColumnRenamed("scaledfeatures", "features")
    dsSmartCar_8.show()
  - 전처리 작업이 끝난 스파크 학습 데이터셋을 LibSVM 형식의 파일로 HDFS의 “/pilot-pit/spark-data/classification/smartCarLibSvm” 경로에 저장
    var dsSmartCar_9 = dsSmartCar_8.select("label", "features")
    dsSmartCar_9.write.format("libsvm").save("/pilot-pjt/spark-data/classification/smartCarLibSVM")
  - 제플린의 스파크ML 컨텍스트로 해당 파일을 다시 로드
    val dsSmartCar_10 = spark.read.format("libsvm").load("/pilot-pjt/spark-data/classification/smartCarLibSVM")
    dsSmartCar_10.show(5)
  - 레이블과 피처의 인덱서를 만들고, 전체 데이터셋을 학습(Training)과 테스트(Test) 데이터로 나누는 코드를 실행한다.
    val labelIndexer = new StringIndexer().setInputCol("label").setOutputCol("indexedLabel").fit(dsSmartCar_10)
    val featureIndexer = new VectorIndexer().setInputCol("features").setOutputCol("indexedFeatures").fit(dsSmartCar_10)
    val Array(trainingData, testData) = dsSmartCar_10.randomSplit(Array (0.7, 0.3))
  -  랜덤 포레스트 머신러닝을 위한 파라미터를 설정한 후, 스파크ML 파이프라인을 만들고 Training 데이터셋으로 모델을 학습시킨다. 
     랜덤 포레스트의 모델은 파일럿 프로젝트 특성상 5개의 트리로만 만든다. (저사양은 3개)
    val rf = new RandomForestClassifier().setLabelCol("indexedLabel").setFeaturesCol("indexedFeatures").setNumTrees(3)
    val labelConverter = new IndexToString().setInputCol("prediction").setOutputCol("predictedLabel").setLabels(labelIndexer.labels)
    val pipeline = new Pipeline().setStages(Array(labelIndexer, featureIndexer, rf , labelConverter))
    val model = pipeline.fit(trainingData) 
  - 모델 학습이 성공적으로 끝나면 다음 코드로 랜덤 포레스트 모델의 설명력을 확인해 본다. 총 3개의 트리가 만들어졌고 각 변수에 정의된 디시전 값을 확인할 수 있다.
    val rfModel = model.stages(2).asInstanceOf[RandomForestClassificationModel]
    println(s"RandomForest Model Description : \n ${rfModel.toDebugString}")
  -  모델 학습이 성공적으로 끝나면 테스트 데이터로 모델의 정확도를 확인해 보기 위한 평가기를 실행
    val predictions = model.transform(testData)
    predictions.select("predictedLabel", "label", "features").show(5)
    val evaluator = new MulticlassClassificationEvaluator().setLabelCol("indexedLabel").setPredictionCol("prediction").setMetricName ("accuracy")
    val accuracy = evaluator.evaluate(predictions)
  - 테스트 데이터로 예측 정확도를 확인해 본다. 
    println(s"@ Accuracy Rate = ${(accuracy)}")		Accuracy Rate = 0.890305374586013
    println(s"@ Error Rate = ${(1.0 - accuracy)}")		Error Rate = 0.109694625413987
  - 다음 코드로 스마트카의 정상/비정상 예측에 대한 Confusion Matrix를 확인할 수 있다.
    val results = model.transform(testData).select("features", "label", "prediction")
    val predictionAndLabels = results.select($"prediction",$"label").as[(Double, Double)].rdd
    val bMetrics = new BinaryClassificationMetrics(predictionAndLabels)
    val mMetrics = new MulticlassMetrics(predictionAndLabels)
    val labels = mMetrics.labels

    println("Confusion Matrix:") 
    println(mMetrics.confusionMatrix)
    ####
    Confusion Matrix:
    873438.0  10902.0   
    120060.0  189478.0 
    정상을 정상으로 판단: 873438건(정답 - True Positive)
    비정상을 정상으로 판단: 10902건(1종 오류 - False Positive)
    정상을 비정상으로 판단: 120060건(2종 오류 - False Negative)
    비정상을 비정상으로 판단: 189478건(정답 - True Negative)

## 
- Precision(정밀도) : 모델이 True라고 예측한 결과 중에서 실제 True인 결과 비율
- Recall(재현율) : 실제 True인 결과 중에서 모델이 True라고 예측한 결과 비율
- F1-Score : Precision과 Recall의 조화 평균
    //Precision(정밀도)
    labels. foreach { rate =>
    println(s"@ Precision Rate($rate) = " + mMetrics.precision(rate))}
    //Recall(재현율)
    labels.foreach { rate => 
    println(s"Recall Rate($rate) = " + mMetrics.recall(rate))}
    //F1-Score
    labels.foreach { rate =>
    println(s"F1 - Score($rate) =" + mMetrics.fMeasure(rate))}


 # 실습 (머하웃과 스파크ML을 이용한 군집 - 스마트카 고객 정보 분석
   : 사용될 데이터셋은 “스마트카 고객 마스터 정보"로 하이브의 External 영역에 SmartCar Master 테이블이다.
    데이터셋에는 스마트카의 차량번호, 차량용량, 차량모델 정보와 스마트카 사용자의 성별, 나이, 결혼 여부, 직업, 거주지역 정보들이 있다. 
    군집분석은 이러한 속성 정보를 벡터화하고 유사도 및 거리를 계산해 데이터의 새로운 군집을 발견하는 마이닝 기법이다.
  - 스마트카 데이터셋 가공
    insert overwrite local directory '/home/pilot-pjt/mahout-data/clustering/input'
    ROW FORMAT DELIMITED
    FIELDS TERMINATED BY ' '
    select 
       car_number,
       case 
          when (car_capacity < 2000) then '소형'
          when (car_capacity < 3000) then '중형'
          when (car_capacity < 4000) then '대형'
      end as car_capacity,
      case
          when ((2016-car_year) <= 4)  then 'NEW' 
          when ((2016-car_year) <= 8)  then 'NORMAL' 
          else 'OLD'
      end as car_year ,
      car_model,
      sex as owner_sex,
      floor (cast(age as int) * 0.1 ) * 10 as owner_age,
      marriage as owner_marriage,
      job as owner_job,
      region as owner_region
    from smartcar_master 

  - “스마트카 사용자 마스터" 데이터셋이 정상적으로 만들어졌는지 확인한다. 
    $ more /home/pilot-pjt/mahout-data/clustering/input/*
  -  머하웃의 Canopy 분석의 입력 데이터로 사용하기 위해 HDFS 상에 /pilot-pit/mahout/clustering/input/ 경로를
     생성하고 앞서 생성한 “스마트카 사용자 마스터" 데이터인 “000000_0" 파일의 이름을 "smartcar_master.txt"로 변경해 HDFS에 저장한다.
    $ hdfs dfs -mkdir -p /pilot-pjt/mahout/clustering/input
    $ cd /home/pilot-pjt/mahout-data/clustering/input
    $ mv 000000_0 smartcar_master.txt
    $ hdfs dfs -put smartcar_master.txt /pilot-pjt/mahout/clustering/input
  - HDFS에 정상적으로 적재됐는지 휴의 파일 브라우저로 확인(/pilot-pit/mahout/clustering/input/)
  - FTP 클라이언트인 파일질라를 실행해 Server02에 접속
    머하웃 작업 경로: /home/pilot-pit/mahout-data
    C://예제소스/bigdata2nd-master/CH07/bigdata.smartcar.mahout-1.0.jar 파일을 /home/pilot-pit/mahout-data에 업로드
  - 텍스트 형식의 “스마트카 사용자 마스터" 파일을 시퀀스 파일로 변환한다. 변환 대상은 앞서 HDFS에 저장해둔 
    /pilot-pit/mahout/clustering/input/smartcar_master.txt 파일이고, 
    변환 결과는 HDFS의 pilot-pit/mahout/clustering/output/seq에 생성된다.
    $ hadoop jar /home/pilot-pjt/mahout-data/bigdata.smartcar.mahout-1.0.jar 
      com.wikibook.bigdata.smartcar.mahout.TextToSequence /pilot-pjt/mahout/clustering/input/smartcar_master.txt 
      /pilot-pjt/mahout/clustering/output/seq
  - 시퀀스 파일의 내용을 확인하기 위해 다음의 HDFS 명령을 이용할 수 있다.
    $ hdfs dfs -text /pilot-pjt/mahout/clustering/output/seq/part-m-00000
  - 해당 시퀀스 파일을 로우별(차량번호)로 n-gram 기반의 TF(Term Frequency) 가중치가 반영된 벡터 데이터로 변환한다. 
    n-gram의 벡터 모델은 단어의 분류와 빈도 수를 측정하는 알고리즘 정도로 이해하자. 
    여기서는 차량번호별 각 항목의 단어를 분리해 벡터화하기 위해 사용하겠다. 다음 명령을 실행해 
    스마트카 마스터 데이터를 다차원의 공간 벡터로 변환해 HDFS의 pilot-pit/mahout/clustering/output/vec에 생성한다.
    $ mahout seq2sparse -i /pilot-pjt/mahout/clustering/output/seq -o /pilot-pjt/mahout/clustering/output/vec -wt tf -s 5 -md 3 -ng 2 -x 85 --namedVector
    <** 적용된 옵션에 대한 설명>
      wt: 단어 빈도 가중치 방식
      md: 최소 문서 출현 횟수
      ng: ngrams 최댓값
      namedVector: 네임벡터 데이터 생성

  - Canopy 군집분석으로 최적의 군집 개수를 파악하기 위해서는 센트로이드로부터 거리를 나타내는 t1, 2 옵션을 바꿔가며 
    반복적인 군집분석을 수행해야 한다. 다음과 같은 명령으로 첫 번째 Canopy 군집분석을 실행해 본다.
    $ mahout canopy -i /pilot-pjt/mahout/clustering/output/vec/tf-vectors/ -o /pilot-pjt/mahout/clustering/canopy/out 
      -dm org.apache.mahout/common.distance.SquaredEuclideanDistanceMeasure -t1 50 -t2 45 -ow
  <**적용된 옵션 설명>
   i : 벡터 파일 경로
   o : 출력 결과 경로
   dm : 군집 거리 측정 알고리즘
   th: 거리1
   t2: 거리값 2
  <추가 설명>
   : Canopy 군집분석에서는 ti, t2의 길이가 t1 > t2 이어야 하고, 중심점으로부터 "t2"의 반경 안에 있는 데이터는 해당 군집의 
     데이터로 확정되며, “t2"와 "t1" 사이의 데이터는 다른 군집 영역에 다시 포함되어 다른 군집의 데이터로 취급될 수 있다.

  - 군집 분석 결과 확인
    $ mahout clusterdump -i /pilot-pjt/mahout/clustering/canopy/out/clusters-*-final








